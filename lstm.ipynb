{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm.ipynb","provenance":[],"mount_file_id":"1z-vXs11V1Ipvs03rq8Gt616M5p8fRvdK","authorship_tag":"ABX9TyPu48PrDyxA/zrFgRC/bkLO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pmDdFPgRzhf-","colab_type":"code","colab":{}},"source":["import pickle\n","import numpy as np\n","import pandas as pd\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugHcYbBp-UFI","colab_type":"code","colab":{}},"source":["# p = pd.read_csv(\"drive/My Drive/pos_final.csv\")\n","# p[\"tag\"] = 1\n","# n = pd.read_csv(\"drive/My Drive/neg_final.csv\")\n","# n[\"tag\"] = 0\n","\n","# dp = pd.DataFrame(p[\"content\"].astype(str))\n","# dn = pd.DataFrame(n[\"content\"].astype(str))\n","\n","# x_p = dp[\"content\"]\n","# x_n = dn[\"content\"]\n","\n","# y_p = p[\"tag\"]\n","# y_n = n[\"tag\"]   \n","\n","# x = pd.concat([x_p, x_n])\n","# y = pd.concat([y_p, y_n])\n","\n","# labels = list(y.unique())\n","# labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAg3sPmQz6ut","colab_type":"code","colab":{}},"source":["  # 出現太少的詞，你可以選擇不看，只留出現次數最高的2000(num_words)\n","  # tok = Tokenizer(num_words=20000)\n","  # tok.fit_on_texts(x)\n","\n","  # x = tok.texts_to_sequences(x)\n","  # x_pad = pad_sequences(x, maxlen=128)\n","  # y = np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSSauwrZ4Nc4","colab_type":"code","colab":{}},"source":["# x_pad.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSVUzmz9zoE0","colab_type":"code","colab":{}},"source":["# 匯入資料\n","# 檔案的資料中，特徵為evaluation, 類別為label.\n","def load_data():\n","    # df = pd.read_csv(filepath)\n","\n","    p = pd.read_csv(\"drive/My Drive/pos_final.csv\")\n","    p[\"tag\"] = 1\n","    n = pd.read_csv(\"drive/My Drive/neg_final.csv\")\n","    n[\"tag\"] = 0\n","\n","    dp = pd.DataFrame(p[\"content\"].astype(str))\n","    dn = pd.DataFrame(n[\"content\"].astype(str))\n","\n","    x_p = dp[\"content\"]\n","    x_n = dn[\"content\"]\n","\n","    y_p = p[\"tag\"]\n","    y_n = n[\"tag\"]   \n","\n","    x = pd.concat([x_p, x_n])\n","    y = pd.concat([y_p, y_n])\n","    \n","    # labels = list(y.unique())\n","    \n","    label_dictionary = {'正面': 1, '負面': 0}\n","    with open('drive/My Drive/label_dict.pk', 'wb') as f:\n","        pickle.dump(label_dictionary, f)\n","    # ------------------------------------------------------------------------\n","    # 預處理 1. 先把文字化成數字    \n","    # 出現太少的詞，你可以選擇不看，只留出現次數最高的2000(num_words)\n","    tok = Tokenizer(num_words=20000)\n","    tok.fit_on_texts(x)\n","\n","    x = tok.texts_to_sequences(x)\n","    x_pad = pad_sequences(x, maxlen=128)\n","    y = np.array(y)\n","    print(x_pad)\n","    print(len(x_pad))\n","    print(y)\n","    print(len(y))\n","\n","    # Saving Dictionary\n","    # with open('drive/My Drive/tokenizer.pk', 'wb') as handle:\n","    # pickle.dump(tok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","    return x_pad, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yuzZqpx0d-y","colab_type":"code","colab":{}},"source":["# 建立深度學習模型， Embedding + LSTM + Softmax.\n","def create_LSTM(n_units, input_shape, output_dim):\n","    x, y = load_data()\n","    model = Sequential()\n","    model.add(Embedding(input_dim=20001, output_dim=output_dim,\n","                        input_length=input_shape, mask_zero=True))\n","    model.add(LSTM(n_units))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(2, activation='softmax'))\n","    model.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n","\n","    plot_model(model, to_file='./model_lstm.png', show_shapes=True)\n","    model.summary()\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLe2hIhL0gVM","colab_type":"code","colab":{}},"source":["# 模型訓練\n","def model_train(input_shape, filepath, model_save_path):\n","\n","    # 將資料集分為訓練集和測試集，佔比為9:1\n","    # input_shape = 100\n","    x, y = load_data()\n","    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.1, random_state = 42)\n","\n","    # 模型輸入引數，需要自己根據需要調整\n","    n_units = 256\n","    batch_size = 256\n","    epochs = 100\n","    output_dim = 128\n","\n","    # 模型訓練\n","    lstm_model = create_LSTM(n_units, input_shape, output_dim)\n","\n","    callbacks = [\n","        EarlyStopping(patience=3, restore_best_weights=True),\n","    #     ModelCheckpoint(\"nlp.h5\", save_best_only=True)\n","    ]\n","\n","    lstm_model.fit(train_x, \n","                   train_y, \n","                   epochs=epochs, \n","                   batch_size=batch_size, \n","                   verbose=1,\n","                   validation_split=0.1,\n","                   callbacks=callbacks)\n","\n","    # 模型儲存\n","    lstm_model.save(model_save_path)\n","\n","    result = lstm_model.evaluate(test_x, test_y)\n","    print(\"測試集結果為:\", result[1])\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKxDJezV0mxM","colab_type":"code","outputId":"c5d8410e-301e-444c-e183-c0d58049bb87","executionInfo":{"status":"ok","timestamp":1590995567975,"user_tz":-480,"elapsed":530332,"user":{"displayName":"吳書澔","photoUrl":"","userId":"13847943732103572605"}},"colab":{"base_uri":"https://localhost:8080/","height":854}},"source":["if __name__ == '__main__':\n","    filepath = \"drive/My Drive/\"\n","    input_shape = 128\n","    model_save_path = 'drive/My Drive/lstm_model.h5'\n","    model_train(input_shape, filepath, model_save_path)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[   0    0    0 ...   68  514  353]\n"," [   0    0    0 ...  109  437    9]\n"," [   0    0    0 ...  675   17    3]\n"," ...\n"," [   0    0    0 ... 4258  138 1171]\n"," [   0    0    0 ...    0  128   46]\n"," [   0    0    0 ...    3  234 1687]]\n","66900\n","[1 1 1 ... 0 0 0]\n","66900\n","[[   0    0    0 ...   68  514  353]\n"," [   0    0    0 ...  109  437    9]\n"," [   0    0    0 ...  675   17    3]\n"," ...\n"," [   0    0    0 ... 4258  138 1171]\n"," [   0    0    0 ...    0  128   46]\n"," [   0    0    0 ...    3  234 1687]]\n","66900\n","[1 1 1 ... 0 0 0]\n","66900\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 128, 128)          2560128   \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 256)               394240    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 256)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 514       \n","=================================================================\n","Total params: 2,954,882\n","Trainable params: 2,954,882\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","212/212 [==============================] - 130s 612ms/step - loss: 0.2066 - accuracy: 0.9202 - val_loss: 0.1193 - val_accuracy: 0.9557\n","Epoch 2/100\n","212/212 [==============================] - 127s 601ms/step - loss: 0.0744 - accuracy: 0.9747 - val_loss: 0.1346 - val_accuracy: 0.9525\n","Epoch 3/100\n","212/212 [==============================] - 121s 573ms/step - loss: 0.0508 - accuracy: 0.9834 - val_loss: 0.1371 - val_accuracy: 0.9537\n","Epoch 4/100\n","212/212 [==============================] - 123s 579ms/step - loss: 0.0364 - accuracy: 0.9881 - val_loss: 0.1620 - val_accuracy: 0.9502\n","210/210 [==============================] - 8s 37ms/step - loss: 0.1102 - accuracy: 0.9590\n","測試集結果為: 0.9590433239936829\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VEIQHZJiP0g_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}